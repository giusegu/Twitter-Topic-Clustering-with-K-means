# -*- coding: utf-8 -*-
"""practica_grupal_2_APAU.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11JtWsb86b87tcGXPIea80-h2889We7Ui

# Paso 1: Setup general

Bloque básico de instalación de librerías y herramientas básicas necesarias para la ejecución. Este bloque DEBE EJECUTARSE SIEMPRE AL PRINCIPIO.
"""

!pip install PyDrive
!pip install fastparquet

import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.patches as mpatches
import statistics

from sklearn import metrics
from sklearn.cluster import DBSCAN
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

from collections import Counter
from itertools import groupby
from mpl_toolkits.mplot3d import Axes3D
from scipy import stats

from fastparquet import ParquetFile
from fastparquet import write

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

sns.set_theme()
sns.set_context("paper")

"""# Paso 2: Conexión con Google Drive para recuperar los ficheros con los datos

Autorice el acceso con su ID de Google, pegue el enlace que aparece y presione "Enter"
"""

"""
Authenticate and create the PyDrive client
"""
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

from google.colab import drive
drive.mount('/content/drive')

"""# Paso 3: Importación de funciones auxiliares

Definición de las funciones auxiliares requeridas
"""

"""
load_npy_data: Función que permite cargar los datos almacenados en un fichero npy
"""
def load_npy_data(npy_file):
  dir = '/content/'+npy_file
  data = pd.DataFrame(np.load(dir))
  return data

"""
load_txt_data: Función que permite cargar los datos almacenados en un fichero txt
"""
def load_txt_data(txt_file):
  dir = '/content/'+txt_file
  data = pd.read_csv(dir, sep=",")
  return data

"""
load_csv_data: Función que permite cargar los datos almacenados en un fichero csv
"""
def load_csv_data(csv_file):
  dir = '/content/'+csv_file
  data = pd.read_csv(dir, header=None)
  return data

"""
save_file: Función que permite salvar datos
"""
def save_file(name, file):
  write(name, file, compression='GZIP')

"""
set_size_letters(): Función que permite modificar el tamaño letra a utilizar en los plots
"""
def set_size_letters(title, x_name, y_name, title_size = 20, x_size = 18, y_size = 18, active_legend = True, legend_size = 14):

  """
  Parameters:
    title (string): titulo del plot a representar

    x_name (string): nombre del eje x

    y_name (string): nombre del eje y

    active_legend (bool): indica si mostramos la leyenda o no. Por defecto True

    x_size, y_size, legend_size: tamaño de fuente de eje x, y, leyenda

  """

  plt.title(title, fontsize=title_size)
  plt.xlabel(x_name, fontsize=x_size)
  plt.ylabel(y_name, fontsize=y_size)
  if (active_legend == True):
    plt.legend(fontsize=legend_size)

"""
PlotDistancesToKnearestNeighbor: Función que devuelve la distancia con el K-vecino más cercano
"""
def PlotDistancesToKnearestNeighbor(data_vector, K):

  """
  Parameters:
    data_vector (numpy.ndarray): array tipo embedding cuyo
        shape es (n_ejemplo,n_muestras_por_ejemplo)

    k: posición del k-esimo vecino más cercano

  """

  nbrs = NearestNeighbors(n_neighbors=K).fit(data_vector)
  distances, indices = nbrs.kneighbors(data_vector)
  distances = np.sort(distances, axis=0)
  distances = distances[:,K-1]
  plt.figure(figsize=(10,8))
  set_size_letters(f"Distancias al K-vecino más cercano (K={K})",
                   f"Points sorted according to distance of the {K}-th nearest neighbor",
                   f"{K}-th nearest neighbor distance")
  plt.plot(distances)
  #
  #plt.plot(min_dist)

"""
nearest_neighbor_distance: Función que devuelve la distancia con el vecino más cercano
"""
def nearest_neighbor_distance (data_vector, name_clustering):

  """
  Parameters:
    data_vector (numpy.ndarray): array tipo embedding cuyo
        shape es (n_ejemplo,n_muestras_por_ejemplo)

    name_clustering (string): nombre de los datos representados, se utiliza
        para el titulo del plot realizado

  """

  nbrs = NearestNeighbors(n_neighbors=2).fit(data_vector)
  distances, indices = nbrs.kneighbors(data_vector)

  min_dist = np.sort(distances[:,1])

  plt.figure(figsize=(10,8))
  set_size_letters(f"Distancia al vecino más cercano: datos {name_clustering}", "nº muestra", "epsilon")
  plt.plot(min_dist)

"""
dbScan_nsamples: Función que reliza barrido dbscan con calculo de silueta
"""
def dbScan_nsamples(data, epsilon_vect, min_samples = 30, include_noise = False):
  """
  Parameters:

    data (numpy.ndarray): array tipo embedding cuyo
        shape es (n_ejemplo,n_muestras_por_ejemplo)

    epsilon_vect (list): lista con los valores de epsilon
        para realizar el barrido (distancia mínima entre elementos
        para formar un cluster)

    min_samples (int): numero minimo de muestras para generar un cluster

    include_noise (bool): permite seleccionar si se quiere incluir el cluster
        de ruido en el cálculo de la silueta o no. Por defecto no se incluye.
        Se debe incluir cuando únicamente se detecta un cluster, para poder
        realizar la evaluación.
  """

  for epsilon in epsilon_vect:

    db = DBSCAN(eps=epsilon, min_samples=min_samples).fit(data)

    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    core_samples_mask[db.core_sample_indices_] = True
    labels = db.labels_

    # Number of clusters in labels, ignoring noise if present.
    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise_ = list(labels).count(-1)

    silhouette_data = np.copy(data)
    noise_points = np.where(labels == -1)

    # delete noise cluster
    if (include_noise == False):
      silhouette_data = np.delete(silhouette_data, noise_points, axis=0)
      silhouette_labels = np.delete(labels, noise_points, axis=0)
    else:
      silhouette_labels = np.copy(labels)

    #calculate silhouette_score
    silhouette_avg = silhouette_score(silhouette_data, silhouette_labels)
    print(f"Epsilon = {epsilon}")
    print(f"N clusters: {n_clusters_}, Nº_noise_points = {n_noise_}")
    print(f"silhouette_score = {silhouette_avg} \r\n")

"""
ApplyDBScanToData: Función que permite seleccionar DBSCAN
"""
def ApplyDBScanToData (samples, epsilon, min_samples = 30):

  """
  Parameters:

    samples (numpy.ndarray): array tipo embedding cuyo
        shape es (n_ejemplo,n_muestras_por_ejemplo)

    epsilon: int con el valor de epsilon (distancia mínima entre elementos
        para formar un cluster)

    min_samples (int): numero minimo de muestras para generar un cluster

    include_noise (bool): permite seleccionar si se quiere incluir el cluster
        de ruido en el cálculo de la silueta o no. Por defecto no se incluye.
        Se debe incluir cuando únicamente se detecta un cluster, para poder
        realizar la evaluación.

  Return:

    labels: lista con el cluster al que pertenece cada ejemplo de data. Cluster
        -1 significa ruido.
  """


  db = DBSCAN(eps=epsilon, min_samples=min_samples).fit(samples)

  core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
  core_samples_mask[db.core_sample_indices_] = True
  labels = db.labels_

  # Number of clusters in labels, ignoring noise if present.
  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
  n_noise_ = list(labels).count(-1)

  print(f"\nTest for epsilon = {epsilon}")
  print('Estimated number of clusters: %d' % n_clusters_)
  print('Estimated number of noise points: %d' % n_noise_)
  print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(samples, labels))


  return labels

def Plot2D (samples_2D, title, axes = {'x': 'x', 'y': 'y'}):

  """
  samples_2D (numpy.ndarray): array tipo embedding cuyo
        shape es (num_muestras, 2) ; 2 por las dos coordenadas

  title(string): titulo de la figura que se utiliza para el plot
  """

  df_samples_2D = pd.DataFrame(data=samples_2D, columns=[axes['x'], axes['y']])

  sns.set(font_scale=3)
  sns.set(rc={'figure.figsize':(10,10)})
  sns.relplot(data=df_samples_2D,
              x=axes['x'],
              y=axes['y'],
              height=10, legend="full", palette="bright")

  set_size_letters(title,
                   axes['x'],
                   axes['y'],
                   active_legend = False)
  plt.axis('equal')

def plot_2D_centroid_labels(df_centroids, ax):
    for index, row in df_centroids.iterrows():
      ax.text(row[0]+.5, row[1], 'C'+str(row[2].astype(int)), fontsize = 20, color='black', weight='semibold')

def plot_3D_centroid_labels(df_centroids, ax):
    for index, row in df_centroids.iterrows():
      ax.text(row[0]+.5, row[1], row[2], row[3].astype(int), fontsize = 20, color='black', weight='semibold')

def Plot2D_WithLabels (samples_2D, labels, title, axes = {'x': 'x', 'y': 'y'}, palette="bright", centroids_2D=None):

  """
  samples_2D (numpy.ndarray): array tipo embedding cuyo
        shape es (num_muestras, 2) ; 2 por las dos coordenadas

  labels (array): etiqueta perteneciente a cada muestra.

  title(string): titulo de la figura que se utiliza para el plot
  """

  # First we create the dataframe
  df_samples_2D_labeled = pd.DataFrame(data=samples_2D, columns=[axes['x'], axes['y']])

  # Then we add the labels column
  df_samples_2D_labeled['label'] = labels.tolist()

  if centroids_2D is not None:
    labels_centroid_aux = np.arange(len(centroids_2D), dtype=int)
    df_centroids_2D_labeled = pd.DataFrame(data=centroids_2D, columns=[axes['x'], axes['y']])
    df_centroids_2D_labeled['label'] = labels_centroid_aux.tolist()

  sns.set(font_scale=3)
  sns.set(rc={'figure.figsize':(10,10)})
  sns.relplot(data=df_samples_2D_labeled,
              x=axes['x'],
              y=axes['y'],
              hue="label", height=10, legend="full", palette=palette)

  if centroids_2D is not None:
    # First we plot the centroids
    sns.scatterplot(data=df_centroids_2D_labeled,
              x=axes['x'],
              y=axes['y'],
              hue="label",
              legend=False, palette=palette, s=100)

    # Then we plot their labels
    plot_2D_centroid_labels(df_centroids_2D_labeled, plt.gca())

  set_size_letters(title,
                   axes['x'],
                   axes['y'],
                   active_legend = False)
  plt.axis('equal')

def Plot3D (samples_3D, title, axes = {'x': 'x', 'y': 'y', 'z': 'z'}):

  """
  samples_3D (numpy.ndarray): array tipo embedding cuyo
        shape es (num_muestras, 3) ; 3 por las tres coordenadas

  title(string): titulo de la figura que se utiliza para el plot
  """

  df_samples_3D = pd.DataFrame(data=samples_3D, columns=[axes['x'], axes['y'], axes['z']])

  fig = px.scatter_3d(df_samples_3D,
                      x=axes['x'],
                      y=axes['y'],
                      z=axes['z'])

  fig.update_layout(scene_aspectmode='data')

  fig.update_layout(
    title_font_size=20,
    title={
        'text': title,
        'y': 0.9,
        'x': 0.05,
        'xanchor': 'left',
        'yanchor': 'top'})

  # tight layout
  fig.update_layout(
      autosize = True, #False,
      #width = 1200,
      #height = 800,
      margin = dict(l=50, r=0, b=10, t=30))

  fig.show()

def Plot3D_WithLabels (samples_3D, labels, title, axes = {'x': 'x', 'y': 'y', 'z': 'z'}, centroids_3D=None):

  """
  samples_3D (numpy.ndarray): array tipo embedding cuyo
        shape es (num_muestras, 3) ; 3 por las tres coordenadas

  labels (array): etiqueta perteneciente a cada muestra.

  title(string): titulo de la figura que se utiliza para el plot
  """

  # First we create the dataframe
  df_samples_3D_labeled = pd.DataFrame(data=samples_3D, columns=[axes['x'], axes['y'], axes['z']])

  # Then we add the labels column
  df_samples_3D_labeled['label'] = labels.tolist()
  df_samples_3D_labeled['label'] = df_samples_3D_labeled["label"].astype(str)

  if centroids_3D is not None:
    # Same applies for centroids when these are provided as an argument
    labels_centroid_aux = np.arange(len(centroids_3D), dtype=int)
    df_centroids_3D_labeled = pd.DataFrame(data=centroids_3D, columns=[axes['x'], axes['y'], axes['z']])
    df_centroids_3D_labeled['label'] = labels_centroid_aux.tolist()

    # We create an additional column with the dot size used for each type of sample
    size_no_centroid = np.ones(len(samples_3D)) * 10 # for regular samples
    size_centroid = np.ones(len(centroids_3D)) * 50 # for centroids
    size_col = np.append(size_no_centroid, size_centroid) # new col to be added to the dataframe

    # We also create another additional column with the labels for each type of sample
    no_es_centroide_aux = [' '] * len(samples_3D) # empty label for regular samples
    es_centroide_aux = []
    for i in range(len(centroids_3D)):
      es_centroide_aux.append('C%d' % i) # Ci label for centroid i
    centroid_col = no_es_centroide_aux + es_centroide_aux # new col to be added to the dataframe

    # Next we concatenate both dataframes: first, regular samples, then, centroids
    df_samples_and_centroids = df_samples_3D_labeled.append(df_centroids_3D_labeled, ignore_index=True)

    # We add the new column with the labels distinguishing regular samples from centroids
    df_samples_and_centroids['centroid'] = centroid_col

    # New column is re-casted as a string column
    df_samples_and_centroids['centroid'] = df_samples_and_centroids['centroid'].astype(str)

    # We add the new column with the corresponding size for both regular samples and centroids
    df_samples_and_centroids['size'] = size_col

    # We ensure that the 'label' column is numeric since we will sort the dataframe upon this one
    df_samples_and_centroids['label'] = pd.to_numeric(df_samples_and_centroids['label'])

    # We finally sort the dataframe by the 'label' column in ascending order
    df_samples_and_centroids_sorted = df_samples_and_centroids.sort_values(by=['label'], ascending=True)

    # And plot both the samples and their corresponding centroids
    fig = px.scatter_3d(df_samples_and_centroids_sorted, x=axes['x'], y=axes['y'], z=axes['z'], text='centroid', size='size', color='label')
  else:
    # We ensure that the 'label' column is numeric since we will sort the dataframe upon this one
    df_samples_3D_labeled['label'] = pd.to_numeric(df_samples_3D_labeled['label'])

    # We finally sort the dataframe by the 'label' column in ascending order
    df_samples_and_centroids_sorted = df_samples_3D_labeled.sort_values(by=['label'], ascending=True)

    fig = px.scatter_3d(df_samples_and_centroids_sorted, x=axes['x'], y=axes['y'], z=axes['z'], color='label', size=np.ones(len(samples_3D))) #, color_continuous_scale='delta')

  fig.update_traces(textposition='top center')
  fig.update_layout(scene_aspectmode='data')
  fig.update_layout(uniformtext_minsize=60)

  fig.update_layout(title_font_size=20,
                    title={
                    'text': title,
                    'y': 0.9,
                    'x': 0.05,
                    'xanchor': 'left',
                    'yanchor': 'top'})
  # tight layout
  fig.update_layout(autosize = True, margin = dict(l=50, r=0, b=10, t=30))
  fig.show()

"""
Representacion pca2d dbscan: Función que permite obtener la representacion pca2d dbscan
"""
def pca2d_clustering_dbscan (data_vector, labels, clustering_name):

  """
  data_vector (numpy.ndarray): array tipo embedding cuyo
        shape es (n_ejemplo,n_muestras_por_ejemplo)

  labels (array): clusters al que pertenece cada muestra. En la representación
        se suma `1' al label indicado (ruido pasa a ser cluster 0)

  clustering_name(string): nombre del clustering, se utiliza para el plot
  """

  pca = PCA(n_components=2)
  pca.fit(data_vector)

  pca_data_vector = pca.transform(data_vector)

  values_labels, k = np.unique(labels, return_counts=True)
  keys = values_labels + 1
  keys = list(map(str,keys))

  pca_clustered_values = dict.fromkeys(keys,None)
  for i in range (len(labels)):
    key = str(labels[i]+1)
    pca_clustered_values[key] = np.append(pca_clustered_values[key], pca_data_vector[i])

  #delete none value
  for k in range (len(keys)):
    pca_clustered_values[keys[k]] = np.delete(pca_clustered_values[keys[k]],[0])
    pca_clustered_values[keys[k]] = np.reshape(pca_clustered_values[keys[k]], (-1, 2))

  labels_aux = np.reshape(labels, (-1,1))
  labels_aux = labels_aux +1

  pca_data_vector_labeled = np.append(pca_data_vector, labels_aux, axis=1)

  clustered_pca_df = pd.DataFrame(data=pca_data_vector_labeled, columns=["pca0", "pca1", "label"])

  sns.set(font_scale=3)
  sns.set(rc={'figure.figsize':(10,10)})
  sns.relplot(data=clustered_pca_df, x="pca0", y="pca1", hue="label", height=10, legend="full", palette="bright")

  set_size_letters(f"PCA 2D: clustering {clustering_name}", "PCA0", "PCA1", active_legend = False)
  plt.axis('equal')

"""
pca2d_no_clustering: Función que permite obtener representación PCA en 2D
"""
def pca2d_no_clustering (data_vector, labels, clustering_name):

  """
  data_vector (numpy.ndarray): array tipo embedding cuyo
        shape es (n_ejemplo,n_muestras_por_ejemplo)

  labels (array): clusters al que pertenece cada muestra. En la representación
        se suma `1' al label indicado (ruido pasa a ser cluster 0)

  clustering_name(string): nombre del clustering, se utiliza para el plot
  """

  pca = PCA(n_components=2)
  pca.fit(data_vector)

  pca_data_vector = pca.transform(data_vector)

  values_labels, k = np.unique(labels, return_counts=True)
  keys = values_labels + 1
  keys = list(map(str,keys))

  pca_clustered_values = dict.fromkeys(keys,None)
  for i in range (len(labels)):
    key = str(labels[i]+1)
    pca_clustered_values[key] = np.append(pca_clustered_values[key], pca_data_vector[i])

  #delete none value
  for k in range (len(keys)):
    pca_clustered_values[keys[k]] = np.delete(pca_clustered_values[keys[k]],[0])
    pca_clustered_values[keys[k]] = np.reshape(pca_clustered_values[keys[k]], (-1, 2))

  labels_aux = np.reshape(labels, (-1,1))
  labels_aux = labels_aux +1

  pca_data_vector_labeled = np.append(pca_data_vector, labels_aux, axis=1)

  clustered_pca_df = pd.DataFrame(data=pca_data_vector_labeled,
                                  columns=["pca0", "pca1", "label"])

  sns.set(font_scale=3)
  sns.set(rc={'figure.figsize':(10,10)})
  sns.relplot(data=clustered_pca_df,
              x="pca0",
              y="pca1", height=10, legend="full", palette="bright")

  set_size_letters(f"PCA 2D: datos {clustering_name}", "PCA0", "PCA1", active_legend = False)
  plt.axis('equal')

"""
pca3d_clustering_dbscan: Función que permite obtener la representación PCA3D DBSCAN
"""
def pca3d_clustering_dbscan(data_vector, labels, clustering_name):

  """
  data_vector (numpy.ndarray): array tipo embedding cuyo
        shape es (n_ejemplo,n_muestras_por_ejemplo)

  labels (array): clusters al que pertenece cada muestra. En la representación
        se suma `1' al label indicado (ruido pasa a ser cluster 0)

  clustering_name(string): nombre del clustering, se utiliza para el plot
  """


  pca_3d = PCA(n_components=3)
  pca_3d.fit(data_vector)

  pca_3d_data_vector = pca_3d.transform(data_vector)

  values_labels, k = np.unique(labels, return_counts=True)
  keys = values_labels +1
  keys = list(map(str,keys))

  pca_3d_clustered_values = dict.fromkeys(keys,None)
  for i in range (len(labels)):
    key = str(labels[i]+1)
    pca_3d_clustered_values[key] = np.append(pca_3d_clustered_values[key], pca_3d_data_vector[i])

  #delete none value
  for k in range (len(keys)):
    pca_3d_clustered_values[keys[k]] = np.delete(pca_3d_clustered_values[keys[k]],[0])
    pca_3d_clustered_values[keys[k]] = np.reshape(pca_3d_clustered_values[keys[k]], (-1, 3))

  labels_aux = np.reshape(labels, (-1,1))
  labels_aux = labels_aux + 1

  pca_3d_data_vector_labeled = np.append(pca_3d_data_vector, labels_aux, axis=1)

  clustered_pca_3d_df = pd.DataFrame(data=pca_3d_data_vector_labeled, columns=["pca0", "pca1", "pca2", "label"])

  lab_aux = values_labels +1
  lab_aux = lab_aux.tolist()


  df = clustered_pca_3d_df.copy()
  df["label"] = df["label"].astype(str)
  fig = px.scatter_3d(df, x='pca0', y='pca1', z='pca2',
                color='label')
  fig.update_layout(scene_aspectmode='data')

  fig.update_layout(
    title_font_size=20,
    title={
        'text': f"PCA 3D: clustering {clustering_name}",
        'y':0.9,
        'x':0.05,
        'xanchor': 'left',
        'yanchor': 'top'})

  # tight layout
  fig.update_layout(
      autosize=False,
      width=1200,
      height=800,
      margin=dict(l=50, r=0, b=10, t=30))

  fig.show()

"""
calculate_centroids: Función que permite el cálculo de los centroides
"""
def calculate_centroids(data_vector, labels):

  """
  parameters
      data_vector (numpy.ndarray): array tipo embedding cuyo
            shape es (n_ejemplo,n_muestras_por_ejemplo)

      labels (array): clusters al que pertenece cada muestra. En la representación
            se suma `1' al label indicado (ruido pasa a ser cluster 0)

  return
      centroids (numpy.ndarray): array donde se almacenan las coordenadas
            de los centroides calculados. Shape: (n_clusters, n_features)
            Donde n_clusters es clusters + noise_cluster
  """

  values_labels, k = np.unique(labels, return_counts=True)
  keys = values_labels +1
  keys = list(map(str,keys))

  n_samples, n_col = data_vector.shape
  fpb_clustered_values = dict.fromkeys(keys,None)
  for i in range (len(labels)):
    key = str(labels[i]+1)
    fpb_clustered_values[key] = np.append(fpb_clustered_values[key], data_vector[i])

  #delete none value
  for k in range (len(keys)):
    fpb_clustered_values[keys[k]] = np.delete(fpb_clustered_values[keys[k]],[0])
    fpb_clustered_values[keys[k]] = np.reshape(fpb_clustered_values[keys[k]], (-1, n_col))

  centroids = np.zeros([len(keys), n_col])
  for n_clust in range (len(keys)):
    for col in range (n_col-1):
      data=fpb_clustered_values[str(n_clust)][:,col]
      centroids[n_clust, col] = np.mean(data, axis=0)
  return centroids

"""
pca2d_clustering_dbscan_with_centroids: Función que permite la representacion PCA2D clustering con centroides
"""
def pca2d_clustering_dbscan_with_centroids(data_vector, centroids, labels, clustering_name):

  """

  parameters
      data_vector (numpy.ndarray): array tipo embedding cuyo
            shape es (n_ejemplo,n_muestras_por_ejemplo)

      centroids (numpy.ndarray): array con las coordenadas
            de los centroides. Shape: (n_clusters, n_features)
            Donde n_clusters es clusters + noise_cluster

      labels (array): clusters al que pertenece cada muestra. En la representación
            se suma `1' al label indicado (ruido pasa a ser cluster 0)

      clustering_name(string): nombre del clustering, se utiliza para el plot

  Extra:
      si los datos presentan mas de 2 dimensiones se realiza la transformación
      PCA.

  """

  if data_vector.shape[1] > 2:
    pca = PCA(n_components=2)
    pca.fit(data_vector)

    pca_data_vector = pca.transform(data_vector)
  else:
    pca_data_vector = data_vector

  if centroids.shape[1] > 2:
    pca_centroids = pca.transform(centroids)
  else:
    pca_centroids = centroids


  values_labels, k = np.unique(labels, return_counts=True)
  keys = values_labels +1
  keys = list(map(str,keys))

  pca_clustered_values = dict.fromkeys(keys,None)
  for i in range (len(labels)):
    key = str(labels[i]+1)
    pca_clustered_values[key] = np.append(pca_clustered_values[key], pca_data_vector[i])

  #delete none value
  for k in range (len(keys)):
    pca_clustered_values[keys[k]] = np.delete(pca_clustered_values[keys[k]],[0])
    pca_clustered_values[keys[k]] = np.reshape(pca_clustered_values[keys[k]], (-1, 2))

  labels_aux = np.reshape(labels, (-1,1))
  labels_aux = labels_aux +1

  pca_data_vector_labeled = np.append(pca_data_vector, labels_aux, axis=1)

  labels_centroid_aux = np.arange(len(pca_centroids))
  labels_centroid_aux = np.reshape(labels_centroid_aux, (-1,1))

  pca_centroids_labeled = np.append(pca_centroids, labels_centroid_aux, axis=1)

  pca_clustered_df = pd.DataFrame(data=pca_data_vector_labeled, columns=["pca0", "pca1", "label"])
  pca_centroid_df = pd.DataFrame(data=pca_centroids_labeled, columns=["pca0", "pca1", "label"])


  sns.set(rc={'figure.figsize':(18,15)})
  sns.set(font_scale=1.5)
  ax = sns.scatterplot(data=pca_clustered_df, x="pca0", y="pca1", hue="label", legend='full', palette="bright", s=20)
  ax = sns.scatterplot(data=pca_centroid_df, x="pca0", y="pca1", hue="label", legend=False, palette="bright", s=100)

  def label_point(x, y, val, ax):
      a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)
      for i, point in a.iterrows():
          ax.text(point['x']+.02, point['y'], str(point['val']))

  label_point(pca_centroid_df.pca0, pca_centroid_df.pca1, pca_centroid_df.label, plt.gca())
  set_size_letters(f"PCA 2D: clustering {clustering_name}", "PCA0", "PCA1")
  plt.axis('equal')

"""
pca2d_clustering_dbscan_with_centroids_no_noise: Función que permite obtener la representación PCA2D clustering DBSCAN con centroides y sin ruido
"""
def pca2d_clustering_dbscan_with_centroids_no_noise(data_vector, centroids, labels, clustering_name):

  """

  parameters
      data_vector (numpy.ndarray): array tipo embedding cuyo
            shape es (n_ejemplo,n_muestras_por_ejemplo)

      centroids (numpy.ndarray): array con las coordenadas
            de los centroides. Shape: (n_clusters, n_features)
            Donde n_clusters es clusters + noise_cluster

      labels (array): clusters al que pertenece cada muestra. En la representación
            se suma `1' al label indicado (ruido pasa a ser cluster 0)

      clustering_name(string): nombre del clustering, se utiliza para el plot

  Extra:
      - si los datos presentan mas de 2 dimensiones se realiza la transformación
      PCA.
      - Hay un error que imprime un punto de ruido para poder tener la leyenda
      con los mismos colores que en el caso con ruido. Originalmente se imprimía
      fuera, pero ahora al hacer el plt.axis('equal') no se puede 'sacar' de la
      figura

  """

  if data_vector.shape[1] > 2:
    pca = PCA(n_components=2)
    pca.fit(data_vector)

    pca_data_vector = pca.transform(data_vector)
  else:
    pca_data_vector = data_vector

  if centroids.shape[1] > 2:
    pca_centroids = pca.transform(centroids)
  else:
    pca_centroids = centroids


  values_labels, k = np.unique(labels, return_counts=True)
  keys = values_labels +1
  keys = list(map(str,keys))

  pca_clustered_values = dict.fromkeys(keys,None)
  for i in range (len(labels)):
    key = str(labels[i]+1)
    pca_clustered_values[key] = np.append(pca_clustered_values[key], pca_data_vector[i])

  #delete none value
  for k in range (len(keys)):
    pca_clustered_values[keys[k]] = np.delete(pca_clustered_values[keys[k]],[0])
    pca_clustered_values[keys[k]] = np.reshape(pca_clustered_values[keys[k]], (-1, 2))

  labels_aux = np.reshape(labels, (-1,1))
  labels_aux = labels_aux +1

  pca_data_vector_labeled = np.append(pca_data_vector, labels_aux, axis=1)

  labels_centroid_aux = np.arange(len(pca_centroids))
  labels_centroid_aux = np.reshape(labels_centroid_aux, (-1,1))

  pca_centroids_labeled = np.append(pca_centroids, labels_centroid_aux, axis=1)

  pca_clustered_df = pd.DataFrame(data=pca_data_vector_labeled, columns=["pca0", "pca1", "label"])
  pca_centroid_df = pd.DataFrame(data=pca_centroids_labeled, columns=["pca0", "pca1", "label"])

  #eliminamos las filas de ruido
  pca_clustered_df_no_noise = pca_clustered_df.drop(pca_clustered_df[pca_clustered_df.label == 0].index)
  pca_centroid_df_no_noise = pca_centroid_df.drop(pca_centroid_df[pca_centroid_df.label == 0].index)
  #Añadimos un dato falso muy alejado para mantener los colores
  pca_clustered_df_no_noise = pca_clustered_df_no_noise.append(pd.DataFrame({"pca0":[12], "pca1":[12], "label":[0]}))
  pca_centroid_df_no_noise = pca_centroid_df_no_noise.append(pd.DataFrame({"pca0":[12], "pca1":[12], "label":[0]}))

  #plot
  sns.set(rc={'figure.figsize':(18,15)})
  sns.set(font_scale=1.5)
  ax = sns.scatterplot(data=pca_clustered_df_no_noise, x="pca0", y="pca1", hue="label", legend='full', palette="bright", s=20)
  ax = sns.scatterplot(data=pca_centroid_df_no_noise, x="pca0", y="pca1", hue="label", legend=False, palette="bright", s=100)


  def label_point(x, y, val, ax):
      a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)
      for i, point in a.iterrows():
          ax.text(point['x']+.02, point['y'], str(point['val']))

  pca_centroid_df_no_noise = pca_centroid_df.drop(pca_centroid_df[pca_centroid_df.label == 0].index)
  label_point(pca_centroid_df_no_noise.pca0, pca_centroid_df_no_noise.pca1, pca_centroid_df_no_noise.label, plt.gca())
  set_size_letters(f"PCA 2D: clustering {clustering_name}", "PCA0", "PCA1")
  plt.axis('equal')

"""
plot_clustering_progresion: Función que permite obtener la representación evolución temporal clustering
"""
def plot_clustering_progresion(labels):

  """
  parameters:
      labels (numpy array): array con labels ordenadas de forma temporal.
          El cluster -1 es ruido.
          Ejemplo: array([-1, -1, -1, ...,  9,  9,  9])

  """

  # agrupamos labels
  count_dups = [sum(1 for _ in group) for _, group in groupby(labels)] #número de elementos repetidos

  keys = list(map(str, np.unique(labels).tolist()))
  list_of_data = {}

  for k in keys:
    list_of_data[k] = []

  n_data_act = 0
  n_data_next = 0
  for n in range (len(count_dups)):
    n_data_act = n_data_next
    n_data_next = n_data_next + count_dups[n]
    n_diff = n_data_next - n_data_act
    list_of_data[str(labels[n_data_act])].append((n_data_act,n_diff))

  fig, ax = plt.subplots()
  fig.set_size_inches(18.5, 10.5)
  y1 = 0
  y2 = 1
  for elem in keys:
    ax.broken_barh(list_of_data[elem], (y1, y2), facecolors='tab:blue')
    y1 += 1

  y_ticks = list(map(int, keys))
  y_ticks = [x+1.5 for x in y_ticks]
  tick_labels = []
  for elem in keys:
    tick_labels.append(f"Cluster: {elem}")

  ax.set_yticks(y_ticks)
  ax.set_yticklabels(tick_labels)
  set_size_letters("Evolución temporal clustering", "Nº muestra", "Cluster seleccionado")
  plt.show()

"""
compare_2_clusters: Función que permite comparar 2 clusters (boxplot)
"""
def compare_2_clusters(data1, data2, n_boxplot, measure, ticks_eje_x = None, colors = ['darkturquoise', 'pink'], legend = ["Boxplot_1", "Boxplot_2"]):

  """
    Parameters:
    data1 (list of list): datos a representar boxplot. Se representarán tantos
        boxplot como listas existan dentro de la lista principal.
        Ejemplo: [[6,4,2], [1,2,5,3,2], [2,3,5,1]]

    data2: (list of list): datos a representar boxplot. Se representarán tantos
        boxplot como listas existan dentro de la lista principal.
        Ejemplo: [[6,4,2], [1,2,5,3,2], [2,3,5,1]]

    n_boxplot: número de boxplot a representar

    ticks_eje_x: Permite indicar el nombre de cada boxplot (opcional).
        Por defecto: None

    colors: Permite seleccionar el color de los boxplot.
        El primer parámetro será para data1, el segundo para data2
        Por defecto: ['darkturquoise', 'pink']

    legend: Inidica el nombre de los datos data1 y data2.
        Por defecto: ["Boxplot_1", "Boxplot_2"]

    Return: None

    Function: permite realizar un plot representando el boxplot de los datos1
        y datos2 simultáneamente.
   """

  if ticks_eje_x:
    ticks = ticks_eje_x

    # first boxplot pair

    positions_l = np.array(range(n_boxplot))*3-0.6
    positions_r = np.array(range(n_boxplot))*3+0.6

    fig, ax = plt.subplots()
    bpl = plt.boxplot(data_l, positions=positions_l, sym='', patch_artist=True, widths=0.8)
    bpr = plt.boxplot(data_r, positions=positions_r, sym='', patch_artist=True, widths=0.8)

    for box in bpl['boxes']:
        box.set_facecolor(color = colors[0])

    for box in bpr['boxes']:
        box.set_facecolor(color = colors[1])

    # Para la leyenda
    patch0 = mpatches.Patch(color=colors[0], label=legend[0])
    patch1 = mpatches.Patch(color=colors[1], label=legend[1])

    if ticks:
      plt.xticks(range(0, len(ticks) * 3, 3), ticks)
    plt.legend(handles=[patch0, patch1])
    set_size_letters(f"Busqueda outliers {measure}: {cluster} respecto a cluster_p", "Área", "Diferencia")
    plt.show()

"""# Paso 4: Carga de los datos"""

# La carga se realiza en dos pasos, primero cargamos los datos del dictionario
embeddings = load_npy_data('drive/MyDrive/ficheros_practica_2_apau/EmoEvalEs-embeddings-BETO.npy')

# Secundo, cargamos los tweets
df_tweets = load_csv_data('drive/MyDrive/ficheros_practica_2_apau/APAUtweets.csv')

# Visualizamos los embeddings
embeddings

# Visualizamos los tweets
df_tweets

"""# Paso 5: Reducción de dimensionalidad"""

# Consideramos los embeddings
embeddings

"""### Técnica de reducción de dimensionalidad: PCA

Probamos a reducir las dimensiones de los embeddings de 768 a 2, utilizando PCA
"""

pca2D = PCA(n_components=2)
pca2D.fit(embeddings)
samples_PCA_2D = pca2D.transform(embeddings)
df_samples_PCA_2D = pd.DataFrame(data=samples_PCA_2D, columns=["pca0", "pca1"])
df_samples_PCA_2D

"""Visualización del resultado en 2 Dimensiones"""

title = 'Original data after 2D PCA transform'
axes_PCA_2D = {'x': 'pca0', 'y': 'pca1'}
Plot2D (samples_PCA_2D, title, axes_PCA_2D)

"""Ahora reducimos las dimensiones de los embeddings de 768 a 3, utilizando PCA"""

pca3D = PCA(n_components=3)
pca3D.fit(embeddings)
samples_PCA_3D = pca3D.transform(embeddings)
df_samples_PCA_3D = pd.DataFrame(data=samples_PCA_3D, columns=["pca0", "pca1", "pca2"])
df_samples_PCA_3D

"""Visualización del resultado en 3 Dimensiones"""

title = 'Original data after 3D PCA transform'
axes_PCA_3D = {'x': 'pca0', 'y': 'pca1', 'z': 'pca2'}
Plot3D (samples_PCA_3D, title, axes_PCA_3D)

"""Algoritmo del codo para sacar el número de clusters (Nos da 6, y el profesor dijo que tendría que darnos entre 10 y 12, pero que no ve mal este número)"""

Sum_of_squared_distances = []
K = range(1,12)
for num_clusters in K :
 kmeans = KMeans(n_clusters=num_clusters)
 kmeans.fit(embeddings)
 Sum_of_squared_distances.append(kmeans.inertia_)
plt.plot(K,Sum_of_squared_distances,"bx-")
plt.xlabel("Values of K")
plt.ylabel("Sum of squared distances/Inertia")
plt.title("Elbow Method For Optimal k")
plt.show()

"""### Técnica de reducción de dimensionalidad alternativa: t-SNE

Probamos a reducir las dimensiones de los embeddings de 768 a 2, utilizando PCA
"""

tsne2D = TSNE(n_components=2)
samples_TSNE_2D = tsne2D.fit_transform(embeddings)
df_samples_TSNE_2D = pd.DataFrame(data=samples_TSNE_2D, columns=["tsne0", "tsne1"])
df_samples_TSNE_2D

"""Visualización del resultado en 2 Dimensiones"""

title = 'Original data after 2D t-SNE transform'
axes_TSNE_2D = {'x': 'tsne0', 'y': 'tsne1'}
Plot2D (samples_TSNE_2D, title, axes_TSNE_2D)

"""Ahora reducimos las dimensiones de los embeddings de 768 a 3, utilizando t-SNE"""

tsne3D = TSNE(n_components=3)
samples_TSNE_3D = tsne3D.fit_transform(embeddings)
df_samples_TSNE_3D = pd.DataFrame(data=samples_TSNE_3D, columns=["tsne0", "tsne1", "tsne2"])
df_samples_TSNE_3D

"""Visualización del resultado en 3 Dimensiones"""

title = 'Original data after 3D t-SNE transform'
axes_TSNE_3D = {'x': 'tsne0', 'y': 'tsne1', 'z': 'tsne2'}
Plot3D (samples_TSNE_3D, title, axes_TSNE_3D)

"""# Paso 6: Técnicas de clustering

##DBSCAN

### DBSCAN sobre PCA 2D (clusters: 6, sil: 0.210)
"""

PlotDistancesToKnearestNeighbor(samples_PCA_2D, 20)

min_samples = 20
epsilon_values = [0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75]
aux_labels = {}
for epsilon in epsilon_values:
  aux_labels[epsilon] = ApplyDBScanToData (samples_PCA_2D, epsilon, min_samples=min_samples)

epsilon = 0.2
labels_PCA_2D = aux_labels[epsilon]

title = f'DBScan(eps={epsilon}, MinPts={min_samples}) over 2D PCA transformed data'
Plot2D_WithLabels(samples_PCA_2D, labels_PCA_2D, title, axes_PCA_2D)

"""### DBSCAN sobre PCA 3D (clusters: 7, sil: 0.208)"""

PlotDistancesToKnearestNeighbor(samples_PCA_3D, 20)

min_samples = 20
epsilon_values = [0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 0.8, 0.9, 1]
aux_labels = {}
for epsilon in epsilon_values:
  aux_labels[epsilon] = ApplyDBScanToData (samples_PCA_3D, epsilon, min_samples=min_samples)

epsilon = 0.4
labels_PCA_3D = aux_labels[epsilon]

title = f'DBScan(eps={epsilon}, MinPts={min_samples}) over 3D PCA transformed data'
Plot3D_WithLabels(samples_PCA_3D, labels_PCA_3D, title, axes_PCA_3D)

"""### DBSCAN sobre t-SNE 2D (clusters: 15, sil: 0.390)"""

PlotDistancesToKnearestNeighbor(samples_TSNE_2D, 100)

min_samples = 100
epsilon_values = [4.5, 4.75, 5, 5.25, 5.5, 5.75, 6.25, 6.5, 6.75, 7, 7.25, 7.5, 7.75]
aux_labels = {}
for epsilon in epsilon_values:
  aux_labels[epsilon] = ApplyDBScanToData (samples_TSNE_2D, epsilon, min_samples=min_samples)

epsilon = 6.25
labels_TSNE_2D = aux_labels[epsilon]

title = f'DBScan(eps={epsilon}, MinPts={min_samples}) over 2D t-SNE transformed data'
Plot2D_WithLabels(samples_TSNE_2D, labels_TSNE_2D, title, axes_TSNE_2D)

"""### DBSCAN sobre t-SNE 3D (clusters: 12, sil: 0.269)

"""

PlotDistancesToKnearestNeighbor(samples_TSNE_3D, 100)

min_samples = 100
epsilon_values = [4.5, 4.75, 5, 5.25, 5.5, 5.75, 6.25, 6.5, 6.75, 7, 7.25]
aux_labels = {}
for epsilon in epsilon_values:
  aux_labels[epsilon] = ApplyDBScanToData (samples_TSNE_3D, epsilon, min_samples=min_samples)

epsilon = 5.25
labels_TSNE_3D = aux_labels[epsilon]

title = f'DBScan(eps={epsilon}, MinPts={min_samples}) over 3D t-SNE transformed data'
Plot3D_WithLabels (samples_TSNE_3D, labels_TSNE_3D, title, axes_TSNE_3D)

"""## K-Means

### K-Means sobre PCA 2D (clusters: 10, sil: 0.350/0.400)

Buscamos un valor K para el número de clusters objetivo que resulte más adecuado para nuestros datos transformados vía PCA a 2 dimensiones
"""

from sklearn.metrics import silhouette_score

range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
silhouette_avg = []
for num_clusters in range_n_clusters:
 # initialise kmeans
 kmeans = KMeans(n_clusters=num_clusters)
 kmeans.fit(samples_PCA_2D)
 cluster_labels = kmeans.labels_
 # silhouette score
 silhouette_avg.append(silhouette_score(samples_PCA_2D, cluster_labels))

plt.plot(range_n_clusters,silhouette_avg)
plt.xlabel('Values of K')
plt.ylabel('Silhouette score')
plt.title('Silhouette analysis For Optimal k')
plt.show()

n_clusters = 10
kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(samples_PCA_2D)
labels_kmeans_PCA_2D = kmeans.labels_
labels_kmeans_PCA_2D

centroids_kmeans_PCA_2D = kmeans.cluster_centers_
centroids_kmeans_PCA_2D

title = f'K-means(K={n_clusters}) over 2D PCA transformed data'
plot = Plot2D_WithLabels (samples_PCA_2D, labels_kmeans_PCA_2D, title, axes_PCA_2D, centroids_2D=centroids_kmeans_PCA_2D)

"""### K-Means sobre PCA 3D (clusters: 10, sil: 0.320/0.350)

Buscamos un valor K para el número de clusters objetivo que resulte más adecuado para nuestros datos transformados vía PCA a 3 dimensiones
"""

from sklearn.metrics import silhouette_score

range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
silhouette_avg = []
for num_clusters in range_n_clusters:
 # initialise kmeans
 kmeans = KMeans(n_clusters=num_clusters)
 kmeans.fit(samples_PCA_3D)
 cluster_labels = kmeans.labels_
 # silhouette score
 silhouette_avg.append(silhouette_score(samples_PCA_3D, cluster_labels))

plt.plot(range_n_clusters,silhouette_avg)
plt.xlabel('Values of K')
plt.ylabel('Silhouette score')
plt.title('Silhouette analysis For Optimal k')
plt.show()

n_clusters = 10
kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(samples_PCA_3D)
labels_kmeans_PCA_3D = kmeans.labels_
labels_kmeans_PCA_3D

centroids_kmeans_PCA_3D = kmeans.cluster_centers_
centroids_kmeans_PCA_3D

title = f'K-means(K={n_clusters}) over 3D PCA transformed data'
plot = Plot3D_WithLabels (samples_PCA_3D, labels_kmeans_PCA_3D, title, axes_PCA_3D, centroids_3D=centroids_kmeans_PCA_3D)

"""### K-Means sobre t-SNE 2D (clusters: 12, sil: 0.440) (Muy Buen Resultado)

Buscamos un valor K para el número de clusters objetivo que resulte más adecuado para nuestros datos transformados vía t-SNE a 2 dimensiones
"""

from sklearn.metrics import silhouette_score

range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
silhouette_avg = []
for num_clusters in range_n_clusters:
 # initialise kmeans
 kmeans = KMeans(n_clusters=num_clusters)
 kmeans.fit(samples_TSNE_2D)
 cluster_labels = kmeans.labels_
 # silhouette score
 silhouette_avg.append(silhouette_score(samples_TSNE_2D, cluster_labels))

plt.plot(range_n_clusters,silhouette_avg)
plt.xlabel('Values of K')
plt.ylabel('Silhouette score')
plt.title('Silhouette analysis For Optimal k')
plt.show()

n_clusters = 12
kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(samples_TSNE_2D)
labels_kmeans_TSNE_2D = kmeans.labels_
labels_kmeans_TSNE_2D

centroids_kmeans_TSNE_2D = kmeans.cluster_centers_
centroids_kmeans_TSNE_2D

title = f'K-means(K={n_clusters}) over 2D t-SNE transformed data'
plot = Plot2D_WithLabels (samples_TSNE_2D, labels_kmeans_TSNE_2D, title, axes_TSNE_2D, centroids_2D=centroids_kmeans_TSNE_2D)

"""### K-Means sobre t-SNE 3D (clusters: 12, sil: 0.350) (Buen Resultado)

Buscamos un valor K para el número de clusters objetivo que resulte más adecuado para nuestros datos transformados vía t-SNE a 3 dimensiones
"""

from sklearn.metrics import silhouette_score

range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
silhouette_avg = []
for num_clusters in range_n_clusters:
 # initialise kmeans
 kmeans = KMeans(n_clusters=num_clusters)
 kmeans.fit(samples_TSNE_3D)
 cluster_labels = kmeans.labels_
 # silhouette score
 silhouette_avg.append(silhouette_score(samples_TSNE_3D, cluster_labels))

plt.plot(range_n_clusters,silhouette_avg)
plt.xlabel('Values of K')
plt.ylabel('Silhouette score')
plt.title('Silhouette analysis For Optimal k')
plt.show()

n_clusters = 12
kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(samples_TSNE_3D)
labels_kmeans_TSNE_3D = kmeans.labels_
labels_kmeans_TSNE_3D

centroids_kmeans_TSNE_3D = kmeans.cluster_centers_
centroids_kmeans_TSNE_3D

title = f'K-means(K={n_clusters}) over 3D t-SNE transformed data'
plot = Plot3D_WithLabels (samples_TSNE_3D, labels_kmeans_TSNE_3D, title, axes_TSNE_3D, centroids_3D=centroids_kmeans_TSNE_3D)

"""## EM

### EM sobre PCA 2D (clusters: 10, sil: 0.350/0.400)
"""

from sklearn.metrics import silhouette_score

range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
silhouette_avg = []
for num_clusters in range_n_clusters:
 # initialise kmeans
 gm_2D = GaussianMixture(n_components=num_clusters, random_state=0)
 gm_2D.fit(samples_PCA_2D)
 cluster_labels = gm_2D.predict(samples_PCA_2D)
 # silhouette score
 silhouette_avg.append(silhouette_score(samples_PCA_2D, cluster_labels))

plt.plot(range_n_clusters,silhouette_avg)
plt.xlabel('Values of K')
plt.ylabel('Silhouette score')
plt.title('Silhouette analysis For Optimal k')
plt.show()

n_clusters = 10
gm_2D = GaussianMixture(n_components=n_clusters, random_state=0).fit(samples_PCA_2D)
labels_EM_PCA_2D = gm_2D.predict(samples_PCA_2D)
labels_EM_PCA_2D

centroids_EM_PCA_2D = gm_2D.means_
centroids_EM_PCA_2D

title = f'EM(K={n_clusters}) over 2D PCA transformed data'
plot = Plot2D_WithLabels (samples_PCA_2D, labels_EM_PCA_2D, title, axes_PCA_2D, centroids_2D=centroids_EM_PCA_2D)

"""### EM sobre PCA 3D (clusters: 10, sil: 0.250/0.300)"""

from sklearn.metrics import silhouette_score

range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
silhouette_avg = []
for num_clusters in range_n_clusters:
 # initialise kmeans
 gm_3D = GaussianMixture(n_components=num_clusters, random_state=0)
 gm_3D.fit(samples_PCA_3D)
 cluster_labels = gm_3D.predict(samples_PCA_3D)
 # silhouette score
 silhouette_avg.append(silhouette_score(samples_PCA_3D, cluster_labels))

plt.plot(range_n_clusters,silhouette_avg)
plt.xlabel('Values of K')
plt.ylabel('Silhouette score')
plt.title('Silhouette analysis For Optimal k')
plt.show()

n_clusters = 10
gm_3D = GaussianMixture(n_components=n_clusters, random_state=0).fit(samples_PCA_3D)
labels_EM_PCA_3D = gm_3D.predict(samples_PCA_3D)
labels_EM_PCA_3D

centroids_EM_PCA_3D = gm_3D.means_
centroids_EM_PCA_3D

title = f'EM(K={n_clusters}) over 3D PCA transformed data'
plot = Plot3D_WithLabels (samples_PCA_3D, labels_EM_PCA_3D, title, axes_PCA_3D, centroids_3D=centroids_EM_PCA_3D)

"""### EM sobre t-SNE 2D (clusters: 11, sil: 0.420/0.440)"""

from sklearn.metrics import silhouette_score

range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
silhouette_avg = []
for num_clusters in range_n_clusters:
 # initialise kmeans
 gm_2D = GaussianMixture(n_components=num_clusters, random_state=0)
 gm_2D.fit(samples_TSNE_2D)
 cluster_labels = gm_2D.predict(samples_TSNE_2D)
 # silhouette score
 silhouette_avg.append(silhouette_score(samples_TSNE_2D, cluster_labels))

plt.plot(range_n_clusters,silhouette_avg)
plt.xlabel('Values of K')
plt.ylabel('Silhouette score')
plt.title('Silhouette analysis For Optimal k')
plt.show()

n_clusters = 11
gm_2D = GaussianMixture(n_components=n_clusters, random_state=0).fit(samples_TSNE_2D)
labels_EM_TSNE_2D = gm_2D.predict(samples_TSNE_2D)
labels_EM_TSNE_2D

centroids_EM_TSNE_2D = gm_2D.means_
centroids_EM_TSNE_2D

title = f'EM(K={n_clusters}) over 2D t-SNE transformed data'
plot = Plot2D_WithLabels (samples_TSNE_2D, labels_EM_TSNE_2D, title, axes_TSNE_2D, centroids_2D=centroids_EM_TSNE_2D)

"""### EM sobre t-SNE 3D (clusters: 12, sil: 0.340)"""

from sklearn.metrics import silhouette_score

range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
silhouette_avg = []
for num_clusters in range_n_clusters:
 # initialise kmeans
 gm_3D = GaussianMixture(n_components=num_clusters, random_state=0)
 gm_3D.fit(samples_TSNE_3D)
 cluster_labels = gm_3D.predict(samples_TSNE_3D)
 # silhouette score
 silhouette_avg.append(silhouette_score(samples_TSNE_3D, cluster_labels))

plt.plot(range_n_clusters,silhouette_avg)
plt.xlabel('Values of K')
plt.ylabel('Silhouette score')
plt.title('Silhouette analysis For Optimal k')
plt.show()

n_clusters = 12
gm_3D = GaussianMixture(n_components=n_clusters, random_state=0).fit(samples_TSNE_3D)
labels_EM_TSNE_3D = gm_3D.predict(samples_TSNE_3D)
labels_EM_TSNE_3D

centroids_EM_TSNE_3D = gm_3D.means_
centroids_EM_TSNE_3D

title = f'EM(K={n_clusters}) over 3D t-SNE transformed data'
plot = Plot3D_WithLabels (samples_TSNE_3D, labels_EM_TSNE_3D, title, axes_TSNE_3D, centroids_3D=centroids_EM_TSNE_3D)

"""# Paso 7: Interpretación de los clusters

Para realizar nuestro análisis sobre los resultados obtenidos con las diferentes estrategias de clustering que hemos realizado anteriormente, asignamos a la variable **labels** la variable que contenga el array con la lista de etiquetas o ids de los clusters resultantes para la estrategia en cuyo análisis estemos interesados.

Primero lo haremos con los embeddings de K-Means TSNE_3D
"""

labels = labels_kmeans_TSNE_3D

(unique, counts) = np.unique(labels, return_counts=True)

print('Esta es la distribución de muestras disponibles para cada cluster')
for i in range(len(unique)):
  print('[cluster = %s][%d samples]' % (unique[i], counts[i]))
print('----------------------------')
print('[TODAS][%d samples]\n' % (len(labels)))

"""Recuperamos el conjunto de ids únicos que identifican a nuestros clusters y componemos una lista de claves a partir de la cual crearemos el diccionario"""

values_labels, k = np.unique(labels, return_counts=True)
label_key_list = list(map(str, values_labels))
label_key_list

"""Recordemos que **"df_tweets"** es un dataframe en el que cada fila corresponde a un tweet diferente"""

df_tweets

"""Vamos a añadir a dicho dataframe una nueva columna con la información de la etiqueta correspondiente al cluster al que dicha medida ha sido asignada como resultado del clustering:"""

n_nodes = 1
labels_per_node = np.repeat(labels, n_nodes)

len(labels_per_node)

df_tweets_clustered = df_tweets.copy()            # We create a copy of the original dataframe
df_tweets_clustered["cluster"] = labels_per_node # We add a new column with the corresponding cluster info for each sample
df_tweets_clustered

# Transformamos todos los dataframes de pandas en arrays de numpy para poder realizar operaciones sobre los datos

tweets_cluster_0 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 0]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_1 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 1]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_2 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 2]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_3 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 3]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_4 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 4]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_5 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 5]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_6 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 6]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_7 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 7]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_8 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 8]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_9 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 9]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_10 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 10]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_11 = (df_tweets_clustered.loc[df_tweets_clustered['cluster'] == 11]).drop('cluster', axis = 1).to_numpy()
tweets_cluster_0[0]

"""## Clusters"""

# Creamos una lista vacía en la que irán todas las palabras del cluster 0.

words_in_cluster_0 = []
words_in_cluster_1 = []
words_in_cluster_2 = []
words_in_cluster_3 = []
words_in_cluster_4 = []
words_in_cluster_5 = []
words_in_cluster_6 = []
words_in_cluster_7 = []
words_in_cluster_8 = []
words_in_cluster_9 = []
words_in_cluster_10 = []
words_in_cluster_11 = []

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 0

words_in_cluster_0_array = np.array([])
for x in range(tweets_cluster_0.size):
  words_in_cluster_0_array = np.union1d(words_in_cluster_0_array, tweets_cluster_0[x])
len(words_in_cluster_0_array)

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 1

words_in_cluster_1_array = np.array([])
for x in range(tweets_cluster_1.size):
  words_in_cluster_1_array = np.union1d(words_in_cluster_1_array, tweets_cluster_1[x])
len(words_in_cluster_1_array)

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 2

words_in_cluster_2_array = np.array([])
for x in range(tweets_cluster_2.size):
  words_in_cluster_2_array = np.union1d(words_in_cluster_2_array, tweets_cluster_2[x])
len(words_in_cluster_2_array)

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 3

words_in_cluster_3_array = np.array([])
for x in range(tweets_cluster_3.size):
  words_in_cluster_3_array = np.union1d(words_in_cluster_3_array, tweets_cluster_3[x])
len(words_in_cluster_3_array)

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 4

words_in_cluster_4_array = np.array([])
for x in range(tweets_cluster_4.size):
  words_in_cluster_4_array = np.union1d(words_in_cluster_4_array, tweets_cluster_4[x])
len(words_in_cluster_4_array)

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 5

words_in_cluster_5_array = np.array([])
for x in range(tweets_cluster_5.size):
  words_in_cluster_5_array = np.union1d(words_in_cluster_5_array, tweets_cluster_5[x])
len(words_in_cluster_5_array)

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 6

words_in_cluster_6_array = np.array([])
for x in range(tweets_cluster_6.size):
  words_in_cluster_6_array = np.union1d(words_in_cluster_6_array, tweets_cluster_6[x])
len(words_in_cluster_6_array)

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 7

words_in_cluster_7_array = np.array([])
for x in range(tweets_cluster_7.size):
  words_in_cluster_7_array = np.union1d(words_in_cluster_7_array, tweets_cluster_7[x])
len(words_in_cluster_7_array)

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 8

words_in_cluster_8_array = np.array([])
for x in range(tweets_cluster_8.size):
  words_in_cluster_8_array = np.union1d(words_in_cluster_8_array, tweets_cluster_8[x])
len(words_in_cluster_8_array)

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 9

words_in_cluster_9_array = np.array([])
for x in range(tweets_cluster_9.size):
  words_in_cluster_9_array = np.union1d(words_in_cluster_9_array, tweets_cluster_9[x])
len(words_in_cluster_9_array)

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 10

words_in_cluster_10_array = np.array([])
for x in range(tweets_cluster_10.size):
  words_in_cluster_10_array = np.union1d(words_in_cluster_10_array, tweets_cluster_10[x])
len(words_in_cluster_10_array)

# Creamos un array numpy vacío en el que vamos a añadir todos los elementos de tipo objeto ( o sea, cada tweet individual) de nuestro conjunto de datos relativos al cluster 11

words_in_cluster_11_array = np.array([])
for x in range(tweets_cluster_11.size):
  words_in_cluster_11_array = np.union1d(words_in_cluster_11_array, tweets_cluster_11[x])
len(words_in_cluster_11_array)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_0 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 0

for x in range (len(words_in_cluster_0_array)):
  words = words_in_cluster_0_array[x].split()
  words_in_cluster_0.extend(words)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_1 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 1

for x in range (len(words_in_cluster_1_array)):
  words = words_in_cluster_1_array[x].split()
  words_in_cluster_1.extend(words)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_2 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 2

for x in range (len(words_in_cluster_2_array)):
  words = words_in_cluster_2_array[x].split()
  words_in_cluster_2.extend(words)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_3 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 3

for x in range (len(words_in_cluster_3_array)):
  words = words_in_cluster_3_array[x].split()
  words_in_cluster_3.extend(words)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_4 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 4

for x in range (len(words_in_cluster_4_array)):
  words = words_in_cluster_4_array[x].split()
  words_in_cluster_4.extend(words)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_5 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 5

for x in range (len(words_in_cluster_5_array)):
  words = words_in_cluster_5_array[x].split()
  words_in_cluster_5.extend(words)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_6 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 6

for x in range (len(words_in_cluster_6_array)):
  words = words_in_cluster_6_array[x].split()
  words_in_cluster_6.extend(words)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_7 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 7

for x in range (len(words_in_cluster_7_array)):
  words = words_in_cluster_7_array[x].split()
  words_in_cluster_7.extend(words)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_8 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 8

for x in range (len(words_in_cluster_8_array)):
  words = words_in_cluster_8_array[x].split()
  words_in_cluster_8.extend(words)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_9 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 9

for x in range (len(words_in_cluster_9_array)):
  words = words_in_cluster_9_array[x].split()
  words_in_cluster_9.extend(words)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_10 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 10

for x in range (len(words_in_cluster_10_array)):
  words = words_in_cluster_10_array[x].split()
  words_in_cluster_10.extend(words)

# Atención: ejecutar sólo una vez!!
# Aquí añadimos a la lista vacía words_in_cluster_11 creada anteriormente, todas las palabras presentes en todos los tweets del cluster 11

for x in range (len(words_in_cluster_11_array)):
  words = words_in_cluster_11_array[x].split()
  words_in_cluster_11.extend(words)

words_in_cluster_0
len(words_in_cluster_0)

words_in_cluster_1
len(words_in_cluster_1)

words_in_cluster_2
len(words_in_cluster_2)

words_in_cluster_3
len(words_in_cluster_3)

words_in_cluster_4
len(words_in_cluster_4)

words_in_cluster_5
len(words_in_cluster_5)

words_in_cluster_6
len(words_in_cluster_6)

words_in_cluster_7
len(words_in_cluster_7)

words_in_cluster_8
len(words_in_cluster_8)

words_in_cluster_9
len(words_in_cluster_9)

words_in_cluster_10
len(words_in_cluster_10)

words_in_cluster_11
len(words_in_cluster_11)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 0
words_in_cluster_0 = [item for item in words_in_cluster_0 if not item.isdigit()]
len(words_in_cluster_0)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 1
words_in_cluster_1 = [item for item in words_in_cluster_1 if not item.isdigit()]
len(words_in_cluster_1)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 2
words_in_cluster_2 = [item for item in words_in_cluster_2 if not item.isdigit()]
len(words_in_cluster_2)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 3
words_in_cluster_3 = [item for item in words_in_cluster_3 if not item.isdigit()]
len(words_in_cluster_3)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 4
words_in_cluster_4 = [item for item in words_in_cluster_4 if not item.isdigit()]
len(words_in_cluster_4)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 5
words_in_cluster_5 = [item for item in words_in_cluster_5 if not item.isdigit()]
len(words_in_cluster_5)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 6
words_in_cluster_6 = [item for item in words_in_cluster_6 if not item.isdigit()]
len(words_in_cluster_6)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 7
words_in_cluster_7 = [item for item in words_in_cluster_7 if not item.isdigit()]
len(words_in_cluster_7)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 8
words_in_cluster_8 = [item for item in words_in_cluster_8 if not item.isdigit()]
len(words_in_cluster_8)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 9
words_in_cluster_9 = [item for item in words_in_cluster_9 if not item.isdigit()]
len(words_in_cluster_9)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 10
words_in_cluster_10 = [item for item in words_in_cluster_10 if not item.isdigit()]
len(words_in_cluster_10)

# Eliminamos todos los dígitos numéricos de la lista de palabras relacionadas con el clúster 11
words_in_cluster_11 = [item for item in words_in_cluster_11 if not item.isdigit()]
len(words_in_cluster_11)

words_in_cluster_0

# Instalamos una librería para poder eliminar emoji de las palabras de nuestra lista
!pip install clean-text

from cleantext import clean

for x in range(len(words_in_cluster_0)):
  words_in_cluster_0[x] = clean(words_in_cluster_0[x], no_emoji=True)

for x in range(len(words_in_cluster_1)):
  words_in_cluster_1[x] = clean(words_in_cluster_1[x], no_emoji=True)

for x in range(len(words_in_cluster_2)):
  words_in_cluster_2[x] = clean(words_in_cluster_2[x], no_emoji=True)

for x in range(len(words_in_cluster_3)):
  words_in_cluster_3[x] = clean(words_in_cluster_3[x], no_emoji=True)

for x in range(len(words_in_cluster_4)):
  words_in_cluster_4[x] = clean(words_in_cluster_4[x], no_emoji=True)

for x in range(len(words_in_cluster_5)):
  words_in_cluster_5[x] = clean(words_in_cluster_5[x], no_emoji=True)

for x in range(len(words_in_cluster_6)):
  words_in_cluster_6[x] = clean(words_in_cluster_6[x], no_emoji=True)

for x in range(len(words_in_cluster_7)):
  words_in_cluster_7[x] = clean(words_in_cluster_7[x], no_emoji=True)

for x in range(len(words_in_cluster_8)):
  words_in_cluster_8[x] = clean(words_in_cluster_8[x], no_emoji=True)

for x in range(len(words_in_cluster_9)):
  words_in_cluster_9[x] = clean(words_in_cluster_9[x], no_emoji=True)

for x in range(len(words_in_cluster_10)):
  words_in_cluster_10[x] = clean(words_in_cluster_10[x], no_emoji=True)

for x in range(len(words_in_cluster_11)):
  words_in_cluster_11[x] = clean(words_in_cluster_11[x], no_emoji=True)

# Eliminamos de nuestra lista para el cluster 0 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_0 = [item for item in words_in_cluster_0 if not len(item)<=5]

# Eliminamos de nuestra lista para el cluster 1 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_1 = [item for item in words_in_cluster_1 if not len(item)<=5]

# Eliminamos de nuestra lista para el cluster 2 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_2 = [item for item in words_in_cluster_2 if not len(item)<=5]

# Eliminamos de nuestra lista para el cluster 3 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_3 = [item for item in words_in_cluster_3 if not len(item)<=5]

# Eliminamos de nuestra lista para el cluster 4 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_4 = [item for item in words_in_cluster_4 if not len(item)<=5]

# Eliminamos de nuestra lista para el cluster 5 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_5 = [item for item in words_in_cluster_5 if not len(item)<=5]

# Eliminamos de nuestra lista para el cluster 6 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_6 = [item for item in words_in_cluster_6 if not len(item)<=5]

# Eliminamos de nuestra lista para el cluster 7 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_7 = [item for item in words_in_cluster_7 if not len(item)<=5]

# Eliminamos de nuestra lista para el cluster 8 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_8 = [item for item in words_in_cluster_8 if not len(item)<=5]

# Eliminamos de nuestra lista para el cluster 9 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_9 = [item for item in words_in_cluster_9 if not len(item)<=5]

# Eliminamos de nuestra lista para el cluster 10 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_10 = [item for item in words_in_cluster_10 if not len(item)<=5]

# Eliminamos de nuestra lista para el cluster 11 todas las palabras que tienen una longitud menor o igual a 5,
# para centrarnos en palabras consistentes que puedan representar nuestro cluster
words_in_cluster_11 = [item for item in words_in_cluster_11 if not len(item)<=5]

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 0, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_0)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 0 están todas relacionadas con el mismo tema: la política. Así, la etiqueta del cluster 0 será "elecciones españolas".

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 1, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_1)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 1 están todas relacionadas con el mismo tema: los libros. Así, la etiqueta del cluster 1 será "libros".

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 2, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_2)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 2 están todas relacionadas con el mismo tema: el fútbol. Así, la etiqueta del cluster 2 será "fútbol".

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 3, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_3)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 3 están todas relacionadas con el mismo tema: los libros. Así, la etiqueta del cluster 3 será "libros".
# Aquí, sin embargo, tenemos un problema porque el cluster 1 también está relacionado con los libros, por lo que la división de clusters que se ha hecho no es muy precisa en este caso

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 4, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_4)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 4 están todas relacionadas con el mismo tema: los libros. Así, la etiqueta del cluster 4 será "política".

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 5, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_5)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 5 están todas relacionadas con el mismo tema: el fútbol. Así, la etiqueta del cluster 5 será "fútbol".
# Aquí, sin embargo, tenemos un problema porque el cluster 3 también está relacionado con el fútbol, por lo que la división de clusters que se ha hecho no es muy precisa en este caso

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 6, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_6)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 6 están todas relacionadas con el mismo tema: el medio ambiente. Así, la etiqueta del cluster 6 será "medio ambiente".

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 7, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_7)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 7 NO están todas relacionadas con el mismo tema, entonces
# la división de clusters que se ha hecho no es muy precisa en este caso

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 8, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_8)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 8 están todas relacionadas con el mismo tema: los libros. Así, la etiqueta del cluster 8 será "libros".
# Aquí, sin embargo, tenemos un problema porque el cluster 1 y 3 también está relacionado con los libros, por lo que la división de clusters que se ha hecho no es muy precisa en este caso

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 9, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_9)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 9 están todas relacionadas con el mismo tema: Notre-Dame de Paris. Así, la etiqueta del cluster 9 será "Notre-Dame de Paris".

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 10, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_10)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 10 están todas relacionadas con el mismo tema: Notre-Dame de Paris. Así, la etiqueta del cluster 9 será "Notre-Dame de Paris".
# Aquí, sin embargo, tenemos un problema porque el cluster 9 también está relacionado con Notre-Dame de Paris, por lo que la división de clusters que se ha hecho no es muy precisa en este caso

# Aquí buscamos las 5 palabras más comunes de la lista para el cluster 11, de forma que podamos encontrar una etiqueta para nuestro cluster
from collections import Counter
Counter = Counter(words_in_cluster_11)

most_occur = Counter.most_common(5)
print(most_occur)

# Llegamos a la conclusión de que las palabras más comunes dentro del cluster 11 NO están todas relacionadas con el mismo tema, entonces
# la división de clusters que se ha hecho no es muy precisa en este caso

"""Témas de los tweets (KMEANS sobre t-SNE 3D):


*   Cluster 1: Venezuela / Venezuela libre

*   Cluster 2: Día del libro / Libros

*   Cluster 3: Venezuela / Libros / Notredame

*   Cluster 4: Elecciones Generales / Elecciones / España

*   Cluster 5: Notredame / Catedral / Incendio
*   Cluster 6: La liga / Championsleague / Fútbol


*   Cluster 7: Gretathunberg / Fridays For Future / Cambio Climatico


*   Cluster 8: Championsleague / La liga / Messi


*   Cluster 9: Notredame / Incendio / Catedral


*   Cluster 10: Juego de Tronos / Game of Thrones / Capítulo


*   Cluster 11: Juego de Tronos / Game of Thrones / Capítulo

*   Cluster 12: Elecciones Generales / Venezuela / Elecciones

[Venezuela, Libros, Elecciones, Notredame, Laliga, Gretathunberg, Championsleague, Juego de Tronos, Messi, Fridays for Future]



"""